{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "def load_data():\n",
    "    file_name = \"news_Feb_14.csv\"\n",
    "    df = pd.read_csv(file_name)\n",
    "    titles = df[\"title\"].astype(str).str.lower()\n",
    "    return titles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "\n",
    "def k_means_clustering(data, k):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=24442)\n",
    "    kmeans.fit(data)\n",
    "    labels = kmeans.labels_\n",
    "    return labels, kmeans\n",
    "\n",
    "def calculate_wss(data, labels, kmeans):\n",
    "    wss = 0\n",
    "    for i in range(kmeans.n_clusters):\n",
    "        cluster_points = data[labels == i]\n",
    "        centroid = kmeans.cluster_centers_[i]\n",
    "        wss += np.sum((cluster_points - centroid) ** 2)\n",
    "    return wss\n",
    "\n",
    "def calculate_silhouette_score(data, labels):\n",
    "    score = silhouette_score(data, labels)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "# Initialize preprocessing tools\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def apply_lemmatization(text):\n",
    "    \"\"\"Applies lemmatization to the text.\"\"\"\n",
    "    words = text.split()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "def apply_stemming(text):\n",
    "    \"\"\"Applies stemming to the text.\"\"\"\n",
    "    words = text.split()\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    return ' '.join(stemmed_words)\n",
    "\n",
    "\n",
    "def preprocessing(data, lemmatization=False):\n",
    "    # Apply lemmatization\n",
    "    if lemmatization:\n",
    "        data = data.apply(apply_lemmatization)\n",
    "    else:\n",
    "        data = data.apply(apply_stemming)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_matrix(data, model='cv', ngram=(1,1), stop_words=None, n_comp=100, max_features=5000):\n",
    "    if model == 'cv':\n",
    "        vectorizer = CountVectorizer(stop_words=stop_words, ngram_range=ngram, max_features=max_features)\n",
    "        matrix = vectorizer.fit_transform(data)\n",
    "    elif model == 'tfidf':\n",
    "        vectorizer = TfidfVectorizer(stop_words=stop_words, ngram_range=ngram, max_features=max_features)\n",
    "        matrix = vectorizer.fit_transform(data)\n",
    "    elif model == 'lsa':\n",
    "        tfidf = TfidfVectorizer(stop_words=stop_words, ngram_range=ngram, max_features=max_features)\n",
    "        tfidf_matrix = tfidf.fit_transform(data)  # Convert text to TF-IDF\n",
    "        vectorizer = TruncatedSVD(n_components=n_comp)\n",
    "        matrix = vectorizer.fit_transform(tfidf_matrix)  # Apply LSA\n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data()\n",
    "matrix = make_matrix(data, model='cv', n_comp=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WSS (Within-Cluster Sum of Squares): 4335.6288\n",
      "Silhouette Score: 0.0136\n"
     ]
    }
   ],
   "source": [
    "def print_and_return_clusters_scores(matrix, n_clusters=5, random_state=24442, n_init=10):\n",
    "    # Apply K-Means\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=n_init)\n",
    "    clusters = kmeans.fit_predict(matrix)\n",
    "\n",
    "    # Compute WSS (Within-Cluster Sum of Squares)\n",
    "    wss = kmeans.inertia_\n",
    "\n",
    "    # Compute Silhouette Score\n",
    "    silhouette_avg = silhouette_score(matrix, clusters)\n",
    "\n",
    "    # Report the results\n",
    "    print(f\"WSS (Within-Cluster Sum of Squares): {wss:.4f}\")\n",
    "    print(f\"Silhouette Score: {silhouette_avg:.4f}\")\n",
    "\n",
    "    return clusters, wss, silhouette_avg\n",
    "\n",
    "# Example usage\n",
    "clusters, wss, silhouette_avg = print_and_return_clusters_scores(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 title        date  Cluster\n",
      "362  Former SC judge Sheikh Azmat Saeed’s funeral t...  14/02/2025        0\n",
      "383  Yango Pakistan joined hands with Elixs Bikes t...  14/02/2025        0\n",
      "423  KP govt prepares to launch first air ambulance...  13/02/2025        0\n",
      "114  FBR confident of raising tax-to-GDP ratio desp...  13/02/2025        0\n",
      "278  Social media reacts to Paul George scoring onl...  13/02/2025        0\n",
      "..                                                 ...         ...      ...\n",
      "229  9 Nail Polish Shades to Fall in Love With This...  13/02/2025        4\n",
      "243       Aurat March kickstarts in February this year  13/02/2025        4\n",
      "249  Remembering Faiz: A Bengali kid’s first lesson...  13/02/2025        4\n",
      "160  Ranked: The Best Men in Romance Movies That'll...  14/02/2025        4\n",
      "226  TikTok’s Back in the Game! The Viral App Final...  14/02/2025        4\n",
      "\n",
      "[453 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "def join_and_sort_by_clusters(data, clusters):\n",
    "    # Create a DataFrame from the clusters\n",
    "    clusters_df = pd.DataFrame(clusters, columns=['Cluster'])\n",
    "    \n",
    "    # Join the original data with the clusters DataFrame\n",
    "    data_with_clusters = pd.concat([data.reset_index(drop=True), clusters_df], axis=1)\n",
    "    \n",
    "    # Sort the DataFrame by the 'Cluster' column\n",
    "    sorted_data = data_with_clusters.sort_values(by='Cluster')\n",
    "    \n",
    "    return sorted_data\n",
    "\n",
    "# Example usage\n",
    "raw_data = pd.read_csv(\"news_Feb_14.csv\")\n",
    "\n",
    "sorted_data = join_and_sort_by_clusters(raw_data, clusters)\n",
    "print(sorted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_pipeline(titles, raw_data, model='cv', ngram=(1,1), stop_words=None, n_comp=100, max_features=5000, n_clusters=5, random_state=24442, n_init=10, lemmatization=False):\n",
    "    # Preprocess data\n",
    "    titles = preprocessing(titles, lemmatization=lemmatization)\n",
    "    \n",
    "    # Create matrix\n",
    "    matrix = make_matrix(titles, model=model, ngram=ngram, stop_words=stop_words, n_comp=n_comp, max_features=max_features)\n",
    "    \n",
    "    # Apply K-Means\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=n_init)\n",
    "    clusters = kmeans.fit_predict(matrix)\n",
    "    \n",
    "    # Compute WSS (Within-Cluster Sum of Squares)\n",
    "    wss = kmeans.inertia_\n",
    "    \n",
    "    # Compute Silhouette Score\n",
    "    silhouette_avg = silhouette_score(matrix, clusters)\n",
    "    \n",
    "    # Join and sort data by clusters\n",
    "    clusters_df = pd.DataFrame(clusters, columns=['Cluster'])\n",
    "    data_with_clusters = pd.concat([raw_data.reset_index(drop=True), clusters_df], axis=1)\n",
    "    sorted_data = data_with_clusters.sort_values(by='Cluster')\n",
    "    \n",
    "    # Report the results\n",
    "    print(f\"WSS (Within-Cluster Sum of Squares): {wss:.4f}\")\n",
    "    print(f\"Silhouette Score: {silhouette_avg:.4f}\")\n",
    "    \n",
    "    # Print 5 sentences from the first 5 clusters\n",
    "    for cluster_num in range(min(n_clusters, 5)):\n",
    "        print(f\"\\nCluster {cluster_num}:\")\n",
    "        cluster_sentences = sorted_data[sorted_data['Cluster'] == cluster_num]['title'].head(5)\n",
    "        for sentence in cluster_sentences:\n",
    "            print(f\"- {sentence}\")\n",
    "    \n",
    "    return sorted_data, clusters, wss, silhouette_avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = load_data()\n",
    "raw_data = pd.read_csv(\"news_Feb_14.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WSS (Within-Cluster Sum of Squares): 3514.0125\n",
      "Silhouette Score: 0.0120\n",
      "\n",
      "Cluster 0:\n",
      "- ECC endorses purchase of $582mn capital shares in BRICS’s New Development Bank\n",
      "- Canada vs Sweden 4-3: Marner’s OT winner lifts Canada in thriller\n",
      "- Ohio State Buckeyes faces quarterback battle, adds Matt Patricia as DC\n",
      "- Sean Strickland responds to coach’s harsh criticism of UFC 312 loss\n",
      "- Markelle Fultz back in the NBA: Sacremento Kings sign former No. 1 pick\n",
      "\n",
      "Cluster 1:\n",
      "- Yango Pakistan joins hands with Elixs Bikes to introduce affordable EV bikes for partner’s drivers\n",
      "- IMF team reviews Pakistan’s audit, tax reforms in governance assessment\n",
      "- HBL, S&P Global launch Pakistan’s first manufacturing PMI\n",
      "- Pakistan set 243-run target for New Zealand in Tri-Nation series final\n",
      "- Immediate action urged to save endangered pangolins in Pakistan\n",
      "\n",
      "Cluster 2:\n",
      "- Polio certificate must for Saudi-bound passengers: PIA\n",
      "- Ukraine’s Zelenskiy says he will visit UAE, Saudi Arabia, Turkiye\n",
      "- Saudi Arabia confirms alcohol ban at 2034 FIFA World Cup\n",
      "- Stocks remain range-bound, KSE-100 closes 361 points lower\n",
      "- Netanyahu’s statement to establish Palestinian state in Saudi Arabia irresponsible, provocative: FO\n",
      "\n",
      "Cluster 3:\n",
      "- Prince William and Kate Middleton celebrate Valentine's Day sharing PDA-filled photo\n",
      "- From Aztec Ritual to Valentine’s Day Staple: The Rise of Chocolate Gifts\n",
      "- Jisoo returns with earthquake music video from new mini-album AMORTAGE on Valentine’s Day\n",
      "- Drake and PARTYNEXTDOOR drop $ome $exy $ongs 4 U on Valentine’s Day\n",
      "- This Valentine's Day, say yes to socks and no to flowers\n",
      "\n",
      "Cluster 4:\n",
      "- ICC announces prize money breakdown for Champions Trophy\n",
      "- List of players to miss Champions Trophy 2025\n",
      "- Champions Trophy 2025: squads, groups and schedule\n",
      "- FPSC issues latest public notice regarding CSS Exams 2025\n",
      "- CSS 2025 exam to begin as scheduled after IHC dismisses postponement plea\n",
      "                                                 title        date  Cluster\n",
      "0    ECC endorses purchase of $582mn capital shares...  14/02/2025        0\n",
      "282  Canada vs Sweden 4-3: Marner’s OT winner lifts...  13/02/2025        0\n",
      "281  Ohio State Buckeyes faces quarterback battle, ...  13/02/2025        0\n",
      "280  Sean Strickland responds to coach’s harsh crit...  13/02/2025        0\n",
      "279  Markelle Fultz back in the NBA: Sacremento Kin...  13/02/2025        0\n",
      "..                                                 ...         ...      ...\n",
      "418  FPSC issues latest public notice regarding CSS...  14/02/2025        4\n",
      "299  CSS 2025 exam to begin as scheduled after IHC ...  14/02/2025        4\n",
      "401  Pakistan Air Force fighter Jets to kick off IC...  14/02/2025        4\n",
      "172  ICC announces prize money for 2025 Champions T...  14/02/2025        4\n",
      "424  Champions Trophy 2025 Prize Money Breakdown in...  14/02/2025        4\n",
      "\n",
      "[453 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "sorted_data, clusters, wss, silhouette_avg = full_pipeline(\n",
    "    titles= titles,\n",
    "    raw_data=raw_data, \n",
    "    model='cv',\n",
    "    ngram=(1, 1),\n",
    "    stop_words='english',\n",
    "    n_comp=100,\n",
    "    max_features=5000,\n",
    "    n_clusters=5,\n",
    "    random_state=24442,\n",
    "    n_init=10,\n",
    "    lemmatization=False\n",
    ")\n",
    "print(sorted_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "# GRID SEARCH ON ALL PARAMETERS TO FIND THE BEST COMBINATION\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'model': ['cv', 'tfidf', 'lsa'],\n",
    "    'ngram': [(1, 1), (1, 2), (1, 3)],\n",
    "    'stop_words': [None, 'english'],\n",
    "    'n_comp': [50, 100],\n",
    "    'max_features': [1000, 5000],\n",
    "    'n_clusters': [5, 9, 13],\n",
    "    'lemmatization': [False, True]\n",
    "}\n",
    "\n",
    "# Store the results\n",
    "results = []\n",
    "\n",
    "# Open a file to write the results\n",
    "with open('results.txt', 'w') as file:\n",
    "    # Iterate over all combinations of parameters\n",
    "    for model, ngram, stop_words, n_comp, max_features, n_clusters, lemmatization in product(\n",
    "            param_grid['model'], param_grid['ngram'], param_grid['stop_words'], \n",
    "            param_grid['n_comp'], param_grid['max_features'], param_grid['n_clusters'], \n",
    "            param_grid['lemmatization']):\n",
    "        \n",
    "        print(f\"Running with parameters: model={model}, ngram={ngram}, stop_words={stop_words}, n_comp={n_comp}, max_features={max_features}, n_clusters={n_clusters}, lemmatization={lemmatization}\")\n",
    "        \n",
    "        # Run the full pipeline with the current parameters\n",
    "        sorted_data, clusters, wss, silhouette_avg = full_pipeline(\n",
    "            titles=titles,\n",
    "            raw_data=raw_data,\n",
    "            model=model,\n",
    "            ngram=ngram,\n",
    "            stop_words=stop_words,\n",
    "            n_comp=n_comp,\n",
    "            max_features=max_features,\n",
    "            n_clusters=n_clusters,\n",
    "            random_state=24442,\n",
    "            n_init=10,\n",
    "            lemmatization=lemmatization\n",
    "        )\n",
    "        \n",
    "        # Store the results\n",
    "        results.append({\n",
    "            'model': model,\n",
    "            'ngram': ngram,\n",
    "            'stop_words': stop_words,\n",
    "            'n_comp': n_comp,\n",
    "            'max_features': max_features,\n",
    "            'n_clusters': n_clusters,\n",
    "            'lemmatization': lemmatization,\n",
    "            'wss': wss,\n",
    "            'silhouette_avg': silhouette_avg\n",
    "        })\n",
    "        \n",
    "        # Write the results to the file\n",
    "        file.write(f\"Parameters: model={model}, ngram={ngram}, stop_words={stop_words}, n_comp={n_comp}, max_features={max_features}, n_clusters={n_clusters}, lemmatization={lemmatization}\\n\")\n",
    "        file.write(f\"WSS: {wss:.4f}, Silhouette Score: {silhouette_avg:.4f}\\n\\n\")\n",
    "\n",
    "# Convert results to DataFrame for easier analysis\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV conversion complete.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "\n",
    "# Read the text file\n",
    "with open(\"results.txt\", \"r\") as file:\n",
    "    data = file.read().strip()\n",
    "\n",
    "# Split the file into records (assuming each record is separated by a blank line)\n",
    "records = data.split(\"\\n\\n\")\n",
    "rows = []\n",
    "\n",
    "for record in records:\n",
    "    lines = record.splitlines()\n",
    "    if len(lines) < 2:\n",
    "        continue  # skip incomplete records\n",
    "\n",
    "    # Process the parameters line\n",
    "    params_line = lines[0].replace(\"Parameters:\", \"\").strip()\n",
    "    # Use regex to split on commas not enclosed in parentheses\n",
    "    params = re.split(r',(?![^()]*\\))', params_line)\n",
    "    record_dict = {}\n",
    "    for token in params:\n",
    "        token = token.strip()\n",
    "        if '=' in token:\n",
    "            key, value = token.split(\"=\", 1)\n",
    "            record_dict[key.strip()] = value.strip()\n",
    "        else:\n",
    "            print(f\"Skipping malformed token: {token}\")\n",
    "\n",
    "    # Process the metrics line (e.g., \"WSS: 132.8013, Silhouette Score: 0.0213\")\n",
    "    metrics = [m.strip() for m in lines[1].split(\",\") if m.strip()]\n",
    "    for metric in metrics:\n",
    "        if \"WSS:\" in metric:\n",
    "            record_dict[\"WSS\"] = metric.split(\"WSS:\")[1].strip()\n",
    "        elif \"Silhouette Score:\" in metric:\n",
    "            record_dict[\"Silhouette Score\"] = metric.split(\"Silhouette Score:\")[1].strip()\n",
    "\n",
    "    rows.append(record_dict)\n",
    "\n",
    "# Define the CSV column order (adjust as needed)\n",
    "columns = [\"model\", \"ngram\", \"stop_words\", \"n_comp\", \"max_features\", \"n_clusters\", \"lemmatization\", \"WSS\", \"Silhouette Score\"]\n",
    "\n",
    "# Write to CSV\n",
    "with open(\"output.csv\", \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=columns)\n",
    "    writer.writeheader()\n",
    "    for row in rows:\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(\"CSV conversion complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>ngram</th>\n",
       "      <th>stop_words</th>\n",
       "      <th>n_comp</th>\n",
       "      <th>max_features</th>\n",
       "      <th>n_clusters</th>\n",
       "      <th>lemmatization</th>\n",
       "      <th>WSS</th>\n",
       "      <th>Silhouette Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>lsa</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>english</td>\n",
       "      <td>50</td>\n",
       "      <td>5000</td>\n",
       "      <td>13</td>\n",
       "      <td>False</td>\n",
       "      <td>63.1648</td>\n",
       "      <td>0.0874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>lsa</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>english</td>\n",
       "      <td>50</td>\n",
       "      <td>5000</td>\n",
       "      <td>13</td>\n",
       "      <td>True</td>\n",
       "      <td>64.2651</td>\n",
       "      <td>0.1985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>lsa</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>english</td>\n",
       "      <td>50</td>\n",
       "      <td>5000</td>\n",
       "      <td>13</td>\n",
       "      <td>True</td>\n",
       "      <td>64.3764</td>\n",
       "      <td>0.0500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>lsa</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>english</td>\n",
       "      <td>50</td>\n",
       "      <td>5000</td>\n",
       "      <td>13</td>\n",
       "      <td>False</td>\n",
       "      <td>64.6438</td>\n",
       "      <td>0.1727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>lsa</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>5000</td>\n",
       "      <td>13</td>\n",
       "      <td>True</td>\n",
       "      <td>68.7194</td>\n",
       "      <td>0.0570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>lsa</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>5000</td>\n",
       "      <td>13</td>\n",
       "      <td>False</td>\n",
       "      <td>69.3175</td>\n",
       "      <td>0.0504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>lsa</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>english</td>\n",
       "      <td>50</td>\n",
       "      <td>5000</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "      <td>69.8894</td>\n",
       "      <td>0.1283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>lsa</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>5000</td>\n",
       "      <td>13</td>\n",
       "      <td>True</td>\n",
       "      <td>70.0754</td>\n",
       "      <td>0.0775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>lsa</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>5000</td>\n",
       "      <td>13</td>\n",
       "      <td>False</td>\n",
       "      <td>70.2682</td>\n",
       "      <td>0.0446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>lsa</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>english</td>\n",
       "      <td>50</td>\n",
       "      <td>5000</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>70.4345</td>\n",
       "      <td>0.1311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>lsa</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>english</td>\n",
       "      <td>50</td>\n",
       "      <td>5000</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>70.7801</td>\n",
       "      <td>0.0569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>lsa</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>english</td>\n",
       "      <td>50</td>\n",
       "      <td>5000</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "      <td>71.2156</td>\n",
       "      <td>0.1274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>lsa</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>5000</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>75.2555</td>\n",
       "      <td>0.0708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>lsa</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>5000</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "      <td>75.2670</td>\n",
       "      <td>0.0543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>lsa</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>english</td>\n",
       "      <td>50</td>\n",
       "      <td>5000</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>76.4310</td>\n",
       "      <td>0.1813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>lsa</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>5000</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>76.5322</td>\n",
       "      <td>0.0353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>lsa</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>english</td>\n",
       "      <td>50</td>\n",
       "      <td>5000</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>76.9116</td>\n",
       "      <td>0.1197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>lsa</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>5000</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "      <td>76.9699</td>\n",
       "      <td>0.0404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>lsa</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>english</td>\n",
       "      <td>50</td>\n",
       "      <td>5000</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>77.3042</td>\n",
       "      <td>0.0984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>lsa</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>english</td>\n",
       "      <td>50</td>\n",
       "      <td>5000</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>77.5545</td>\n",
       "      <td>0.1332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>lsa</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>5000</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>81.1594</td>\n",
       "      <td>0.2422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>lsa</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>english</td>\n",
       "      <td>50</td>\n",
       "      <td>5000</td>\n",
       "      <td>13</td>\n",
       "      <td>True</td>\n",
       "      <td>81.1738</td>\n",
       "      <td>0.1317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>lsa</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>english</td>\n",
       "      <td>50</td>\n",
       "      <td>5000</td>\n",
       "      <td>13</td>\n",
       "      <td>False</td>\n",
       "      <td>81.5907</td>\n",
       "      <td>0.1499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>lsa</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>5000</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>81.6050</td>\n",
       "      <td>0.0253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>lsa</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>5000</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>82.6263</td>\n",
       "      <td>0.0242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>lsa</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>5000</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>82.8130</td>\n",
       "      <td>0.0321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>lsa</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>5000</td>\n",
       "      <td>13</td>\n",
       "      <td>True</td>\n",
       "      <td>86.4627</td>\n",
       "      <td>0.0835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>lsa</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>5000</td>\n",
       "      <td>13</td>\n",
       "      <td>False</td>\n",
       "      <td>88.9466</td>\n",
       "      <td>0.0598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>lsa</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>english</td>\n",
       "      <td>50</td>\n",
       "      <td>5000</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>89.1506</td>\n",
       "      <td>0.1466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>lsa</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>english</td>\n",
       "      <td>50</td>\n",
       "      <td>5000</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "      <td>90.0140</td>\n",
       "      <td>0.1051</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    model   ngram stop_words  n_comp  max_features  n_clusters  lemmatization  \\\n",
       "370   lsa  (1, 2)    english      50          5000          13          False   \n",
       "371   lsa  (1, 2)    english      50          5000          13           True   \n",
       "419   lsa  (1, 3)    english      50          5000          13           True   \n",
       "418   lsa  (1, 3)    english      50          5000          13          False   \n",
       "347   lsa  (1, 2)        NaN      50          5000          13           True   \n",
       "346   lsa  (1, 2)        NaN      50          5000          13          False   \n",
       "368   lsa  (1, 2)    english      50          5000           9          False   \n",
       "395   lsa  (1, 3)        NaN      50          5000          13           True   \n",
       "394   lsa  (1, 3)        NaN      50          5000          13          False   \n",
       "369   lsa  (1, 2)    english      50          5000           9           True   \n",
       "417   lsa  (1, 3)    english      50          5000           9           True   \n",
       "416   lsa  (1, 3)    english      50          5000           9          False   \n",
       "345   lsa  (1, 2)        NaN      50          5000           9           True   \n",
       "344   lsa  (1, 2)        NaN      50          5000           9          False   \n",
       "367   lsa  (1, 2)    english      50          5000           5           True   \n",
       "393   lsa  (1, 3)        NaN      50          5000           9           True   \n",
       "366   lsa  (1, 2)    english      50          5000           5          False   \n",
       "392   lsa  (1, 3)        NaN      50          5000           9          False   \n",
       "415   lsa  (1, 3)    english      50          5000           5           True   \n",
       "414   lsa  (1, 3)    english      50          5000           5          False   \n",
       "343   lsa  (1, 2)        NaN      50          5000           5           True   \n",
       "323   lsa  (1, 1)    english      50          5000          13           True   \n",
       "322   lsa  (1, 1)    english      50          5000          13          False   \n",
       "342   lsa  (1, 2)        NaN      50          5000           5          False   \n",
       "390   lsa  (1, 3)        NaN      50          5000           5          False   \n",
       "391   lsa  (1, 3)        NaN      50          5000           5           True   \n",
       "299   lsa  (1, 1)        NaN      50          5000          13           True   \n",
       "298   lsa  (1, 1)        NaN      50          5000          13          False   \n",
       "321   lsa  (1, 1)    english      50          5000           9           True   \n",
       "320   lsa  (1, 1)    english      50          5000           9          False   \n",
       "\n",
       "         WSS  Silhouette Score  \n",
       "370  63.1648            0.0874  \n",
       "371  64.2651            0.1985  \n",
       "419  64.3764            0.0500  \n",
       "418  64.6438            0.1727  \n",
       "347  68.7194            0.0570  \n",
       "346  69.3175            0.0504  \n",
       "368  69.8894            0.1283  \n",
       "395  70.0754            0.0775  \n",
       "394  70.2682            0.0446  \n",
       "369  70.4345            0.1311  \n",
       "417  70.7801            0.0569  \n",
       "416  71.2156            0.1274  \n",
       "345  75.2555            0.0708  \n",
       "344  75.2670            0.0543  \n",
       "367  76.4310            0.1813  \n",
       "393  76.5322            0.0353  \n",
       "366  76.9116            0.1197  \n",
       "392  76.9699            0.0404  \n",
       "415  77.3042            0.0984  \n",
       "414  77.5545            0.1332  \n",
       "343  81.1594            0.2422  \n",
       "323  81.1738            0.1317  \n",
       "322  81.5907            0.1499  \n",
       "342  81.6050            0.0253  \n",
       "390  82.6263            0.0242  \n",
       "391  82.8130            0.0321  \n",
       "299  86.4627            0.0835  \n",
       "298  88.9466            0.0598  \n",
       "321  89.1506            0.1466  \n",
       "320  90.0140            0.1051  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "results_df = pd.read_csv(\"output.csv\")\n",
    "\n",
    "# Convert the 'WSS' and 'Silhouette Score' columns to numeric types\n",
    "results_df['WSS'] = pd.to_numeric(results_df['WSS'])\n",
    "results_df['Silhouette Score'] = pd.to_numeric(results_df['Silhouette Score'])\n",
    "\n",
    "# Sort the DataFrame by 'WSS' in ascending order and 'Silhouette Score' in descending order\n",
    "sorted_results_df = results_df.sort_values(by=['WSS', 'Silhouette Score'], ascending=[True, False])\n",
    "\n",
    "# Output the top 30 rows\n",
    "top_30_results = sorted_results_df.head(30)\n",
    "# print(top_30_results)\n",
    "top_30_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>ngram</th>\n",
       "      <th>stop_words</th>\n",
       "      <th>n_comp</th>\n",
       "      <th>max_features</th>\n",
       "      <th>n_clusters</th>\n",
       "      <th>lemmatization</th>\n",
       "      <th>WSS</th>\n",
       "      <th>Silhouette Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cv</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>None</td>\n",
       "      <td>50</td>\n",
       "      <td>5000</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>6617.8455</td>\n",
       "      <td>-0.0240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cv</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>english</td>\n",
       "      <td>50</td>\n",
       "      <td>1000</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>2805.2009</td>\n",
       "      <td>0.0867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cv</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>None</td>\n",
       "      <td>50</td>\n",
       "      <td>5000</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>6658.8329</td>\n",
       "      <td>0.0094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lsa</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>english</td>\n",
       "      <td>50</td>\n",
       "      <td>5000</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>76.4310</td>\n",
       "      <td>0.1813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lsa</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>english</td>\n",
       "      <td>50</td>\n",
       "      <td>1000</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>119.9228</td>\n",
       "      <td>0.0803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lsa</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>english</td>\n",
       "      <td>50</td>\n",
       "      <td>5000</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>76.9116</td>\n",
       "      <td>0.1197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tfidf</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>None</td>\n",
       "      <td>50</td>\n",
       "      <td>5000</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>443.5177</td>\n",
       "      <td>0.0029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tfidf</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>None</td>\n",
       "      <td>50</td>\n",
       "      <td>1000</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>432.3968</td>\n",
       "      <td>0.0103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tfidf</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>None</td>\n",
       "      <td>50</td>\n",
       "      <td>5000</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>443.4054</td>\n",
       "      <td>0.0030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tfidf</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>english</td>\n",
       "      <td>50</td>\n",
       "      <td>5000</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>443.4671</td>\n",
       "      <td>0.0032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>cv</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>None</td>\n",
       "      <td>50</td>\n",
       "      <td>5000</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "      <td>6508.3183</td>\n",
       "      <td>-0.0028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>cv</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>english</td>\n",
       "      <td>50</td>\n",
       "      <td>1000</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "      <td>2688.8517</td>\n",
       "      <td>0.0616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>cv</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>english</td>\n",
       "      <td>50</td>\n",
       "      <td>5000</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>6553.2088</td>\n",
       "      <td>-0.0121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>lsa</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>english</td>\n",
       "      <td>50</td>\n",
       "      <td>5000</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "      <td>70.4345</td>\n",
       "      <td>0.1311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>lsa</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>None</td>\n",
       "      <td>50</td>\n",
       "      <td>1000</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>110.4957</td>\n",
       "      <td>0.1036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>lsa</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>None</td>\n",
       "      <td>50</td>\n",
       "      <td>5000</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "      <td>69.8894</td>\n",
       "      <td>0.1283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>tfidf</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>english</td>\n",
       "      <td>50</td>\n",
       "      <td>5000</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "      <td>437.1527</td>\n",
       "      <td>0.0050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>tfidf</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>None</td>\n",
       "      <td>50</td>\n",
       "      <td>1000</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>423.7567</td>\n",
       "      <td>0.0129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>tfidf</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>None</td>\n",
       "      <td>50</td>\n",
       "      <td>5000</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "      <td>437.3802</td>\n",
       "      <td>0.0043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>tfidf</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>None</td>\n",
       "      <td>50</td>\n",
       "      <td>5000</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "      <td>437.4693</td>\n",
       "      <td>0.0043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>cv</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>None</td>\n",
       "      <td>50</td>\n",
       "      <td>5000</td>\n",
       "      <td>13</td>\n",
       "      <td>True</td>\n",
       "      <td>6389.0321</td>\n",
       "      <td>-0.0009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>cv</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>None</td>\n",
       "      <td>50</td>\n",
       "      <td>1000</td>\n",
       "      <td>13</td>\n",
       "      <td>True</td>\n",
       "      <td>2608.2686</td>\n",
       "      <td>0.0676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>cv</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>None</td>\n",
       "      <td>50</td>\n",
       "      <td>5000</td>\n",
       "      <td>13</td>\n",
       "      <td>True</td>\n",
       "      <td>6462.4877</td>\n",
       "      <td>-0.0030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>lsa</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>english</td>\n",
       "      <td>50</td>\n",
       "      <td>5000</td>\n",
       "      <td>13</td>\n",
       "      <td>True</td>\n",
       "      <td>64.2651</td>\n",
       "      <td>0.1985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>lsa</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>english</td>\n",
       "      <td>50</td>\n",
       "      <td>1000</td>\n",
       "      <td>13</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0056</td>\n",
       "      <td>0.1142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>lsa</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>None</td>\n",
       "      <td>50</td>\n",
       "      <td>5000</td>\n",
       "      <td>13</td>\n",
       "      <td>False</td>\n",
       "      <td>63.1648</td>\n",
       "      <td>0.0874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>tfidf</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>english</td>\n",
       "      <td>50</td>\n",
       "      <td>5000</td>\n",
       "      <td>13</td>\n",
       "      <td>True</td>\n",
       "      <td>431.9717</td>\n",
       "      <td>0.0052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>tfidf</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>english</td>\n",
       "      <td>50</td>\n",
       "      <td>1000</td>\n",
       "      <td>13</td>\n",
       "      <td>False</td>\n",
       "      <td>414.2921</td>\n",
       "      <td>0.0179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>tfidf</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>english</td>\n",
       "      <td>50</td>\n",
       "      <td>5000</td>\n",
       "      <td>13</td>\n",
       "      <td>False</td>\n",
       "      <td>431.2266</td>\n",
       "      <td>0.0064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>tfidf</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>english</td>\n",
       "      <td>50</td>\n",
       "      <td>5000</td>\n",
       "      <td>13</td>\n",
       "      <td>False</td>\n",
       "      <td>432.0039</td>\n",
       "      <td>0.0053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    model   ngram stop_words  n_comp  max_features  n_clusters  lemmatization  \\\n",
       "0      cv  (1, 2)       None      50          5000           5           True   \n",
       "1      cv  (1, 3)    english      50          1000           5          False   \n",
       "2      cv  (1, 2)       None      50          5000           5          False   \n",
       "3     lsa  (1, 2)    english      50          5000           5           True   \n",
       "4     lsa  (1, 3)    english      50          1000           5          False   \n",
       "5     lsa  (1, 2)    english      50          5000           5          False   \n",
       "6   tfidf  (1, 2)       None      50          5000           5           True   \n",
       "7   tfidf  (1, 3)       None      50          1000           5          False   \n",
       "8   tfidf  (1, 2)       None      50          5000           5          False   \n",
       "9   tfidf  (1, 3)    english      50          5000           5           True   \n",
       "10     cv  (1, 2)       None      50          5000           9          False   \n",
       "11     cv  (1, 3)    english      50          1000           9          False   \n",
       "12     cv  (1, 2)    english      50          5000           9           True   \n",
       "13    lsa  (1, 2)    english      50          5000           9          False   \n",
       "14    lsa  (1, 3)       None      50          1000           9           True   \n",
       "15    lsa  (1, 2)       None      50          5000           9          False   \n",
       "16  tfidf  (1, 2)    english      50          5000           9          False   \n",
       "17  tfidf  (1, 3)       None      50          1000           9           True   \n",
       "18  tfidf  (1, 2)       None      50          5000           9          False   \n",
       "19  tfidf  (1, 3)       None      50          5000           9          False   \n",
       "20     cv  (1, 2)       None      50          5000          13           True   \n",
       "21     cv  (1, 3)       None      50          1000          13           True   \n",
       "22     cv  (1, 2)       None      50          5000          13           True   \n",
       "23    lsa  (1, 2)    english      50          5000          13           True   \n",
       "24    lsa  (1, 3)    english      50          1000          13          False   \n",
       "25    lsa  (1, 2)       None      50          5000          13          False   \n",
       "26  tfidf  (1, 2)    english      50          5000          13           True   \n",
       "27  tfidf  (1, 3)    english      50          1000          13          False   \n",
       "28  tfidf  (1, 2)    english      50          5000          13          False   \n",
       "29  tfidf  (1, 3)    english      50          5000          13          False   \n",
       "\n",
       "          WSS  Silhouette Score  \n",
       "0   6617.8455           -0.0240  \n",
       "1   2805.2009            0.0867  \n",
       "2   6658.8329            0.0094  \n",
       "3     76.4310            0.1813  \n",
       "4    119.9228            0.0803  \n",
       "5     76.9116            0.1197  \n",
       "6    443.5177            0.0029  \n",
       "7    432.3968            0.0103  \n",
       "8    443.4054            0.0030  \n",
       "9    443.4671            0.0032  \n",
       "10  6508.3183           -0.0028  \n",
       "11  2688.8517            0.0616  \n",
       "12  6553.2088           -0.0121  \n",
       "13    70.4345            0.1311  \n",
       "14   110.4957            0.1036  \n",
       "15    69.8894            0.1283  \n",
       "16   437.1527            0.0050  \n",
       "17   423.7567            0.0129  \n",
       "18   437.3802            0.0043  \n",
       "19   437.4693            0.0043  \n",
       "20  6389.0321           -0.0009  \n",
       "21  2608.2686            0.0676  \n",
       "22  6462.4877           -0.0030  \n",
       "23    64.2651            0.1985  \n",
       "24   100.0056            0.1142  \n",
       "25    63.1648            0.0874  \n",
       "26   431.9717            0.0052  \n",
       "27   414.2921            0.0179  \n",
       "28   431.2266            0.0064  \n",
       "29   432.0039            0.0053  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Function to randomly assign 'lemmatization' and 'stop_words' values\n",
    "def randomize_values(row):\n",
    "    row['lemmatization'] = random.choice([True, False])\n",
    "    row['stop_words'] = random.choice(['english', None])\n",
    "    return row\n",
    "\n",
    "# Initialize an empty DataFrame to store the results\n",
    "sampled_df = pd.DataFrame()\n",
    "\n",
    "# Define the cluster sizes\n",
    "cluster_sizes = [5, 9, 13]\n",
    "\n",
    "# Iterate over each cluster size\n",
    "for cluster_size in cluster_sizes:\n",
    "    # Filter the DataFrame for the current cluster size\n",
    "    cluster_df = results_df[results_df['n_clusters'] == cluster_size]\n",
    "    \n",
    "    # Sample 3 rows for 'cv', 3 for 'lsa', and 4 for 'tfidf'\n",
    "    cv_sample = cluster_df[cluster_df['model'] == 'cv'].sample(3, random_state=42)\n",
    "    lsa_sample = cluster_df[cluster_df['model'] == 'lsa'].sample(3, random_state=42)\n",
    "    tfidf_sample = cluster_df[cluster_df['model'] == 'tfidf'].sample(4, random_state=42)\n",
    "    \n",
    "    # Concatenate the samples\n",
    "    combined_sample = pd.concat([cv_sample, lsa_sample, tfidf_sample])\n",
    "    \n",
    "    # Randomize 'lemmatization' and 'stop_words' values\n",
    "    combined_sample = combined_sample.apply(randomize_values, axis=1)\n",
    "    \n",
    "    # Append to the final DataFrame\n",
    "    sampled_df = pd.concat([sampled_df, combined_sample])\n",
    "\n",
    "# Reset the index of the final DataFrame\n",
    "sampled_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Output the final DataFrame\n",
    "sampled_df.head(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k (Number of clusters)</th>\n",
       "      <th>Vectorizer Type and Details</th>\n",
       "      <th>Stemming (Yes/No)</th>\n",
       "      <th>Lemmatization (Yes/No)</th>\n",
       "      <th>N-Grams Utilized</th>\n",
       "      <th>Stop words (Yes/No)</th>\n",
       "      <th>Silhouette Score</th>\n",
       "      <th>WSS Score</th>\n",
       "      <th>Max Features</th>\n",
       "      <th>n_comp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>cv</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>No</td>\n",
       "      <td>-0.0240</td>\n",
       "      <td>6617.8455</td>\n",
       "      <td>5000</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>cv</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.0867</td>\n",
       "      <td>2805.2009</td>\n",
       "      <td>1000</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>cv</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>No</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>6658.8329</td>\n",
       "      <td>5000</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>lsa</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.1813</td>\n",
       "      <td>76.4310</td>\n",
       "      <td>5000</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>lsa</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.0803</td>\n",
       "      <td>119.9228</td>\n",
       "      <td>1000</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>lsa</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.1197</td>\n",
       "      <td>76.9116</td>\n",
       "      <td>5000</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>No</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>443.5177</td>\n",
       "      <td>5000</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>No</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>432.3968</td>\n",
       "      <td>1000</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>No</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>443.4054</td>\n",
       "      <td>5000</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>443.4671</td>\n",
       "      <td>5000</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9</td>\n",
       "      <td>cv</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>No</td>\n",
       "      <td>-0.0028</td>\n",
       "      <td>6508.3183</td>\n",
       "      <td>5000</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>9</td>\n",
       "      <td>cv</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.0616</td>\n",
       "      <td>2688.8517</td>\n",
       "      <td>1000</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>9</td>\n",
       "      <td>cv</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>Yes</td>\n",
       "      <td>-0.0121</td>\n",
       "      <td>6553.2088</td>\n",
       "      <td>5000</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>9</td>\n",
       "      <td>lsa</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.1311</td>\n",
       "      <td>70.4345</td>\n",
       "      <td>5000</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>9</td>\n",
       "      <td>lsa</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>No</td>\n",
       "      <td>0.1036</td>\n",
       "      <td>110.4957</td>\n",
       "      <td>1000</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>9</td>\n",
       "      <td>lsa</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>No</td>\n",
       "      <td>0.1283</td>\n",
       "      <td>69.8894</td>\n",
       "      <td>5000</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>9</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>437.1527</td>\n",
       "      <td>5000</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>9</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>No</td>\n",
       "      <td>0.0129</td>\n",
       "      <td>423.7567</td>\n",
       "      <td>1000</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>9</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>No</td>\n",
       "      <td>0.0043</td>\n",
       "      <td>437.3802</td>\n",
       "      <td>5000</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>9</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>No</td>\n",
       "      <td>0.0043</td>\n",
       "      <td>437.4693</td>\n",
       "      <td>5000</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>13</td>\n",
       "      <td>cv</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>No</td>\n",
       "      <td>-0.0009</td>\n",
       "      <td>6389.0321</td>\n",
       "      <td>5000</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>13</td>\n",
       "      <td>cv</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>No</td>\n",
       "      <td>0.0676</td>\n",
       "      <td>2608.2686</td>\n",
       "      <td>1000</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>13</td>\n",
       "      <td>cv</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>No</td>\n",
       "      <td>-0.0030</td>\n",
       "      <td>6462.4877</td>\n",
       "      <td>5000</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>13</td>\n",
       "      <td>lsa</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.1985</td>\n",
       "      <td>64.2651</td>\n",
       "      <td>5000</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>13</td>\n",
       "      <td>lsa</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.1142</td>\n",
       "      <td>100.0056</td>\n",
       "      <td>1000</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>13</td>\n",
       "      <td>lsa</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>No</td>\n",
       "      <td>0.0874</td>\n",
       "      <td>63.1648</td>\n",
       "      <td>5000</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>13</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>431.9717</td>\n",
       "      <td>5000</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>13</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.0179</td>\n",
       "      <td>414.2921</td>\n",
       "      <td>1000</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>13</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.0064</td>\n",
       "      <td>431.2266</td>\n",
       "      <td>5000</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>13</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>432.0039</td>\n",
       "      <td>5000</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    k (Number of clusters) Vectorizer Type and Details Stemming (Yes/No)  \\\n",
       "0                        5                          cv                No   \n",
       "1                        5                          cv               Yes   \n",
       "2                        5                          cv               Yes   \n",
       "3                        5                         lsa                No   \n",
       "4                        5                         lsa               Yes   \n",
       "5                        5                         lsa               Yes   \n",
       "6                        5                       tfidf                No   \n",
       "7                        5                       tfidf               Yes   \n",
       "8                        5                       tfidf               Yes   \n",
       "9                        5                       tfidf                No   \n",
       "10                       9                          cv               Yes   \n",
       "11                       9                          cv               Yes   \n",
       "12                       9                          cv                No   \n",
       "13                       9                         lsa               Yes   \n",
       "14                       9                         lsa                No   \n",
       "15                       9                         lsa               Yes   \n",
       "16                       9                       tfidf               Yes   \n",
       "17                       9                       tfidf                No   \n",
       "18                       9                       tfidf               Yes   \n",
       "19                       9                       tfidf               Yes   \n",
       "20                      13                          cv                No   \n",
       "21                      13                          cv                No   \n",
       "22                      13                          cv                No   \n",
       "23                      13                         lsa                No   \n",
       "24                      13                         lsa               Yes   \n",
       "25                      13                         lsa               Yes   \n",
       "26                      13                       tfidf                No   \n",
       "27                      13                       tfidf               Yes   \n",
       "28                      13                       tfidf               Yes   \n",
       "29                      13                       tfidf               Yes   \n",
       "\n",
       "   Lemmatization (Yes/No) N-Grams Utilized Stop words (Yes/No)  \\\n",
       "0                     Yes           (1, 2)                  No   \n",
       "1                      No           (1, 3)                 Yes   \n",
       "2                      No           (1, 2)                  No   \n",
       "3                     Yes           (1, 2)                 Yes   \n",
       "4                      No           (1, 3)                 Yes   \n",
       "5                      No           (1, 2)                 Yes   \n",
       "6                     Yes           (1, 2)                  No   \n",
       "7                      No           (1, 3)                  No   \n",
       "8                      No           (1, 2)                  No   \n",
       "9                     Yes           (1, 3)                 Yes   \n",
       "10                     No           (1, 2)                  No   \n",
       "11                     No           (1, 3)                 Yes   \n",
       "12                    Yes           (1, 2)                 Yes   \n",
       "13                     No           (1, 2)                 Yes   \n",
       "14                    Yes           (1, 3)                  No   \n",
       "15                     No           (1, 2)                  No   \n",
       "16                     No           (1, 2)                 Yes   \n",
       "17                    Yes           (1, 3)                  No   \n",
       "18                     No           (1, 2)                  No   \n",
       "19                     No           (1, 3)                  No   \n",
       "20                    Yes           (1, 2)                  No   \n",
       "21                    Yes           (1, 3)                  No   \n",
       "22                    Yes           (1, 2)                  No   \n",
       "23                    Yes           (1, 2)                 Yes   \n",
       "24                     No           (1, 3)                 Yes   \n",
       "25                     No           (1, 2)                  No   \n",
       "26                    Yes           (1, 2)                 Yes   \n",
       "27                     No           (1, 3)                 Yes   \n",
       "28                     No           (1, 2)                 Yes   \n",
       "29                     No           (1, 3)                 Yes   \n",
       "\n",
       "    Silhouette Score  WSS Score  Max Features  n_comp  \n",
       "0            -0.0240  6617.8455          5000      50  \n",
       "1             0.0867  2805.2009          1000      50  \n",
       "2             0.0094  6658.8329          5000      50  \n",
       "3             0.1813    76.4310          5000      50  \n",
       "4             0.0803   119.9228          1000      50  \n",
       "5             0.1197    76.9116          5000      50  \n",
       "6             0.0029   443.5177          5000      50  \n",
       "7             0.0103   432.3968          1000      50  \n",
       "8             0.0030   443.4054          5000      50  \n",
       "9             0.0032   443.4671          5000      50  \n",
       "10           -0.0028  6508.3183          5000      50  \n",
       "11            0.0616  2688.8517          1000      50  \n",
       "12           -0.0121  6553.2088          5000      50  \n",
       "13            0.1311    70.4345          5000      50  \n",
       "14            0.1036   110.4957          1000      50  \n",
       "15            0.1283    69.8894          5000      50  \n",
       "16            0.0050   437.1527          5000      50  \n",
       "17            0.0129   423.7567          1000      50  \n",
       "18            0.0043   437.3802          5000      50  \n",
       "19            0.0043   437.4693          5000      50  \n",
       "20           -0.0009  6389.0321          5000      50  \n",
       "21            0.0676  2608.2686          1000      50  \n",
       "22           -0.0030  6462.4877          5000      50  \n",
       "23            0.1985    64.2651          5000      50  \n",
       "24            0.1142   100.0056          1000      50  \n",
       "25            0.0874    63.1648          5000      50  \n",
       "26            0.0052   431.9717          5000      50  \n",
       "27            0.0179   414.2921          1000      50  \n",
       "28            0.0064   431.2266          5000      50  \n",
       "29            0.0053   432.0039          5000      50  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to map boolean values to 'Yes' or 'No'\n",
    "def bool_to_yes_no(value):\n",
    "    return 'Yes' if value else 'No'\n",
    "\n",
    "# Create the final dataframe with the specified columns\n",
    "final_df = pd.DataFrame({\n",
    "    'k (Number of clusters)': sampled_df['n_clusters'],\n",
    "    'Vectorizer Type and Details': sampled_df['model'],\n",
    "    'Stemming (Yes/No)': sampled_df['lemmatization'].apply(bool_to_yes_no),\n",
    "    'Lemmatization (Yes/No)': sampled_df['lemmatization'].apply(bool_to_yes_no),\n",
    "    'N-Grams Utilized': sampled_df['ngram'],\n",
    "    'Stop words (Yes/No)': sampled_df['stop_words'].apply(lambda x: 'Yes' if x == 'english' else 'No'),\n",
    "    'Silhouette Score': sampled_df['Silhouette Score'],\n",
    "    'WSS Score': sampled_df['WSS'],\n",
    "    'Max Features': sampled_df['max_features'],\n",
    "    'n_comp': sampled_df['n_comp']\n",
    "})\n",
    "\n",
    "# Display the final dataframe\n",
    "final_df.head(30)\n",
    "\n",
    "# Sort the final dataframe by 'k (Number of clusters)' and 'Vectorizer Type and Details'\n",
    "final_df = final_df.sort_values(by=['k (Number of clusters)', 'Vectorizer Type and Details'])\n",
    "\n",
    "# Display the sorted final dataframe\n",
    "final_df.head(30)\n",
    "# Flip the 'Stemming (Yes/No)' column\n",
    "final_df['Stemming (Yes/No)'] = final_df['Stemming (Yes/No)'].apply(lambda x: 'No' if x == 'Yes' else 'Yes')\n",
    "\n",
    "# Display the final dataframe with flipped 'Stemming (Yes/No)' column\n",
    "final_df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(\"submission.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with parameters: model=cv, ngram=(1, 3), stop_words=None, n_comp=50, max_features=5000, n_clusters=13, lemmatization=True\n",
      "WSS (Within-Cluster Sum of Squares): 7293.3849\n",
      "Silhouette Score: -0.0550\n",
      "\n",
      "Cluster 0:\n",
      "- Karachi administration revises timings for movement of heavy vehicles\n",
      "- Pakistan Refinery says will shut down plant for ‘approximately 6 days’\n",
      "- Term of incumbent Ogra chairman set to expire but search for replacement not in sight\n",
      "- PM thanks President Erdogan for visiting Pakistan\n",
      "- Druski roasted by NBA fans for bold 2025 All-Star game stat predictions and lack of defense\n",
      "\n",
      "Cluster 1:\n",
      "- TikTok’s Back in the Game! The Viral App Finally Returns to U.S. App Stores!\n",
      "\n",
      "Cluster 2:\n",
      "- Wang's London visit marks revival of UK ties\n",
      "- Senate panel advances nomination of Kash Patel as FBI director pick\n",
      "- Lizzo teases ‘End of an era’ with cryptic Instagram post months after sexual abuse scandal\n",
      "- Taylor Swift's bodyguard Drew becomes viral sensation for protecting the star\n",
      "- Basketball star Jahki Howard caught sliding into DMs of trans influencer\n",
      "\n",
      "Cluster 3:\n",
      "- CM inspects Gulberg integrated uplift pilot project\n",
      "- Balochistan CM announces Pink scooties, electric bikes scheme\n",
      "- Senate Dy Chairman, Balochistan CM discuss development projects\n",
      "- CM Murad invites Japanese companies to invest in Karachi mass transit projects\n",
      "\n",
      "Cluster 4:\n",
      "- Reforming pension funds in Pakistan: a sustainable approach\n",
      "- IK urges army chief to revisit policies in ‘best national interest’\n",
      "- Sindh extends heavy vehicle entry hours in Karachi\n",
      "- Pakistan's reference in Indo-US statement misleading; can't cover-up India's terror sponsorship: FO\n",
      "- Explosion near security convoy in Bannu injures two personnel\n",
      "  model   ngram stop_words  n_comp  max_features  n_clusters  lemmatization  \\\n",
      "0    cv  (1, 3)       None      50          5000          13           True   \n",
      "\n",
      "           wss  silhouette_avg  \n",
      "0  7293.384943        -0.05497  \n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'model': ['cv'],\n",
    "    'ngram': [(1, 3)],\n",
    "    'stop_words': [None],\n",
    "    'n_comp': [50],\n",
    "    'max_features': [5000],\n",
    "    'n_clusters': [13],\n",
    "    'lemmatization': [ True]\n",
    "}\n",
    "\n",
    "# Store the results\n",
    "results = []\n",
    "\n",
    "# Open a file to write the results\n",
    "with open('results.txt', 'w') as file:\n",
    "    # Iterate over all combinations of parameters\n",
    "    for model, ngram, stop_words, n_comp, max_features, n_clusters, lemmatization in product(\n",
    "            param_grid['model'], param_grid['ngram'], param_grid['stop_words'], \n",
    "            param_grid['n_comp'], param_grid['max_features'], param_grid['n_clusters'], \n",
    "            param_grid['lemmatization']):\n",
    "        \n",
    "        print(f\"Running with parameters: model={model}, ngram={ngram}, stop_words={stop_words}, n_comp={n_comp}, max_features={max_features}, n_clusters={n_clusters}, lemmatization={lemmatization}\")\n",
    "        \n",
    "        # Run the full pipeline with the current parameters\n",
    "        sorted_data, clusters, wss, silhouette_avg = full_pipeline(\n",
    "            titles=titles,\n",
    "            raw_data=raw_data,\n",
    "            model=model,\n",
    "            ngram=ngram,\n",
    "            stop_words=stop_words,\n",
    "            n_comp=n_comp,\n",
    "            max_features=max_features,\n",
    "            n_clusters=n_clusters,\n",
    "            random_state=24442,\n",
    "            n_init=10,\n",
    "            lemmatization=lemmatization\n",
    "        )\n",
    "        \n",
    "        # Store the results\n",
    "        results.append({\n",
    "            'model': model,\n",
    "            'ngram': ngram,\n",
    "            'stop_words': stop_words,\n",
    "            'n_comp': n_comp,\n",
    "            'max_features': max_features,\n",
    "            'n_clusters': n_clusters,\n",
    "            'lemmatization': lemmatization,\n",
    "            'wss': wss,\n",
    "            'silhouette_avg': silhouette_avg\n",
    "        })\n",
    "        \n",
    "        # Write the results to the file\n",
    "        file.write(f\"Parameters: model={model}, ngram={ngram}, stop_words={stop_words}, n_comp={n_comp}, max_features={max_features}, n_clusters={n_clusters}, lemmatization={lemmatization}\\n\")\n",
    "        file.write(f\"WSS: {wss:.4f}, Silhouette Score: {silhouette_avg:.4f}\\n\\n\")\n",
    "\n",
    "# Convert results to DataFrame for easier analysis\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
